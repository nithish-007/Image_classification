{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "import numpy as np \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as f\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./dataset/train\"\n",
    "test_dir = \"./dataset/test\"\n",
    "\n",
    "train_cat_dir = os.path.join(train_dir, \"cats\")\n",
    "train_dog_dir =  os.path.join(train_dir, \"dogs\")\n",
    "\n",
    "def create_dir():\n",
    "    try:\n",
    "        os.makedirs(train_cat_dir, exist_ok= True)\n",
    "        os.makedirs(train_dog_dir, exist_ok= True)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def move_to_respectice_directories(\n",
    "    source_folder,\n",
    "    des_folder,\n",
    "    keyword    \n",
    "    ):\n",
    "    for image_file_path in glob.glob(os.path.join(source_folder, \"**\")):\n",
    "        try:\n",
    "            image_file_name = os.path.basename(image_file_path)\n",
    "            if keyword in image_file_name:\n",
    "                shutil.move(\n",
    "                    os.path.join(source_folder, image_file_name),\n",
    "                    os.path.join(des_folder, image_file_name)\n",
    "                )\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "create_dir()\n",
    "\n",
    "move_to_respectice_directories(train_dir, train_cat_dir, 'cat')\n",
    "move_to_respectice_directories(train_dir, train_dog_dir, \"dog\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The devie use for computation: cuda\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageFolder(train_dir)\n",
    "\n",
    "train_data, val_data, train_label, val_label = train_test_split(\n",
    "    dataset.imgs, dataset.targets,\n",
    "    test_size = 0.1, random_state= 42\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The devie use for computation: {device}\")\n",
    "\n",
    "img_size = 256\n",
    "\n",
    "class DogVSCatDataset(Dataset):\n",
    "    def __init__(self, dataset, transform= None, img_size = 256):\n",
    "        self.dataset = self.is_channel_RGB(dataset)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' \n",
    "        get the length of the dataset\n",
    "        '''\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        get the images and labels, apply the transformation / augmentation\n",
    "        \"\"\"\n",
    "        image = Image.open(self.dataset[idx][0])\n",
    "        label = self.dataset[idx][1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def is_channel_RGB(self, dataset):\n",
    "        ''' \n",
    "        Include only the images that has RGB channel in it. Exclude the rest\n",
    "        '''\n",
    "        dataset_rgb = []\n",
    "        for idx in range(len(dataset)):\n",
    "            if (Image.open(dataset[idx][0]).getbands() == (\"R\", \"G\", \"B\")):\n",
    "                dataset_rgb.append(dataset[idx])\n",
    "\n",
    "        return dataset_rgb\n",
    "    \n",
    "# train transform\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# val transform\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = DogVSCatDataset(train_data, transform= train_transform, img_size= img_size)\n",
    "val_dataset = DogVSCatDataset(val_data, transform= val_transform, img_size= img_size)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= 16, shuffle= True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= 2, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m samples, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m24\u001b[39m))\n\u001b[0;32m      4\u001b[0m grid_imgs \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(samples[:\u001b[38;5;241m24\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "samples, labels = iter(DataLoader(train_dataset, batch_size = 16, shuffle = True)).next()\n",
    "\n",
    "plt.figure(figsize = (16, 24))\n",
    "grid_imgs = torchvision.utils.make_grid(samples[:24])\n",
    "np_grid_imgs = grid_imgs.numpy()\n",
    "\n",
    "# in tensors, image is (batch, width, height), so transpppose it to (width, height, batch) in numpy to show it.\n",
    "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
